{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFB Assignment 1 - Credit Default (Advanced Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By [Omid Shahmirzadi](https://ch.linkedin.com/in/omidshahmirzadi) - Email  [omid.shahmirzadi@epfl.ch](mailto:omid.shahmirzadi@epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we try to predict the probability of default on a credit card account based on the dataset of customer default payments that you already used and are familiar with from Demo 0. From a risk management perspective, the accuracy of the predicted probability of default is more valuable than just a binary prediction (classification) of default itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg)\n",
    "\n",
    "Image source: https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: *Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of information of 30,000 clients with the following 23 features:\n",
    "\n",
    "  * LIMIT_BAL: Amount of the given credit (Taiwanese Dollar) which includes both the individual consumer credit and his/her family (supplementary) credit.  \n",
    "\n",
    "  * SEX: Gender (1 = male; 2 = female).   \n",
    "\n",
    "  * EDUCATION: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).   \n",
    "\n",
    "  * MARRIAGE: Marital status (0 = unknown; 1 = married; 2 = single; 3 = others).   \n",
    "\n",
    "  * AGE: Age (year).   \n",
    "\n",
    "  * PAY1 - PAY6: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: PAY1 = the repayment status in September, 2005; PAY2 = the repayment status in August, 2005; . . .;PAY6 = the repayment status in April, 2005. The measurement scale for the repayment status is: -2 = payment two months in advance; -1 = payment one month in advance; 0 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.  \n",
    "  \n",
    "  * BILL_AMT1-BILL_AMT6: Amount of bill statement (Taiwanese Dollar). BILL_AMT1 = amount of bill statement in September, 2005; BILL_AMT2 = amount of bill statement in August, 2005; . . .; BILL_AMT6 = amount of bill statement in April, 2005.   \n",
    "\n",
    "  * PAY_AMT1-PAY_AMT6: Amount of previous payment (Taiwanese Dollar). PAY_AMT1 = amount paid in September, 2005; PAY_AMT2 = amount paid in August, 2005; . . .; PAY_AMT6 = amount paid in April, 2005. \n",
    "\n",
    "The target variable is given in a separate field called 'default_payment_next_month', where 1 denotes default in the following month and 0 denotes otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might remember, we tried to predict customer defaults using logistic regression. The validation of our results was based on a simple cross validation by splitting data into a training set and a test set. We also used class labels instead of class probabilities. Here we try to elaborate more on this problem, using different models and more statistically sound cross validation schemas and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to import all the data needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Which features need to be one-hot encoded and why? Shall you do that for all categorical features ? Perform the required one-hot encoding and save your pre-processed data into a new data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *get_dummies()* function of Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution :\n",
    "The features that are categorical must be one-hot encoded. Moreovere one set of this features must not be a subset of the same feature. For exemple the feature education is a categorical feature but the set University  is a subset of the set HighSchool since every person at the university graduated from highschool. Therefore in our case the features SEX and MARRIAGE must be one-hot encoded. The others features are either numerical or ordinal with acceptable labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Index(['ID', 'LIMIT_BAL', 'EDUCATION', 'AGE', 'PAY_1', 'PAY_2', 'PAY_3',\n",
      "       'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3',\n",
      "       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2',\n",
      "       'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
      "       'default_payment_next_month', 'SEX_1', 'SEX_2', 'MARRIAGE_0',\n",
      "       'MARRIAGE_1', 'MARRIAGE_2', 'MARRIAGE_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load data into a dataframe\n",
    "\n",
    "data = pd.read_csv('credit_data.csv')\n",
    "#check if there are some NaN values in the Dataframe\n",
    "print(data.isnull().values.any())\n",
    "\n",
    "# One hot encoding of sex and marital status\n",
    "# Applying one-hot encoding for other field doesn't make sense since they are either numeric or ordinal\n",
    "\n",
    "cols_to_transform = ['SEX', 'MARRIAGE']\n",
    "data_with_dummies = pd.get_dummies( data=data, columns = cols_to_transform )\n",
    "\n",
    "print(data_with_dummies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  LIMIT_BAL  EDUCATION  AGE  PAY_1  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6  \\\n",
      "0   1      20000          2   24      2      2     -1     -1     -2     -2   \n",
      "1   2     120000          2   26     -1      2      0      0      0      2   \n",
      "2   3      90000          2   34      0      0      0      0      0      0   \n",
      "3   4      50000          2   37      0      0      0      0      0      0   \n",
      "4   5      50000          2   57     -1      0     -1      0      0      0   \n",
      "\n",
      "      ...      PAY_AMT4  PAY_AMT5  PAY_AMT6  default_payment_next_month  \\\n",
      "0     ...             0         0         0                           1   \n",
      "1     ...          1000         0      2000                           1   \n",
      "2     ...          1000      1000      5000                           0   \n",
      "3     ...          1100      1069      1000                           0   \n",
      "4     ...          9000       689       679                           0   \n",
      "\n",
      "   SEX_1  SEX_2  MARRIAGE_0  MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  \n",
      "0      0      1           0           1           0           0  \n",
      "1      0      1           0           0           1           0  \n",
      "2      0      1           0           0           1           0  \n",
      "3      0      1           0           1           0           0  \n",
      "4      1      0           0           1           0           0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_with_dummies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Obtain correlation of different features compare to the target using Pearson correlation. What are the most correlated ones? Why not only consider them to do the prediction task?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: To compute Pearson correlation of two dataframe columns A and B, do the following:\n",
    "\n",
    "*df['A'].corr(df['B'])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                           -0.013952\n",
      "LIMIT_BAL                    -0.153520\n",
      "EDUCATION                     0.028006\n",
      "AGE                           0.013890\n",
      "PAY_1                         0.324794\n",
      "PAY_2                         0.263551\n",
      "PAY_3                         0.235253\n",
      "PAY_4                         0.216614\n",
      "PAY_5                         0.204149\n",
      "PAY_6                         0.186866\n",
      "BILL_AMT1                    -0.019644\n",
      "BILL_AMT2                    -0.014193\n",
      "BILL_AMT3                    -0.014076\n",
      "BILL_AMT4                    -0.010156\n",
      "BILL_AMT5                    -0.006760\n",
      "BILL_AMT6                    -0.005372\n",
      "PAY_AMT1                     -0.072929\n",
      "PAY_AMT2                     -0.058579\n",
      "PAY_AMT3                     -0.056250\n",
      "PAY_AMT4                     -0.056827\n",
      "PAY_AMT5                     -0.055124\n",
      "PAY_AMT6                     -0.053183\n",
      "default_payment_next_month    1.000000\n",
      "SEX_1                         0.039961\n",
      "SEX_2                        -0.039961\n",
      "MARRIAGE_0                   -0.013158\n",
      "MARRIAGE_1                    0.029775\n",
      "MARRIAGE_2                   -0.030619\n",
      "MARRIAGE_3                    0.009768\n",
      "Name: default_payment_next_month, dtype: float64\n",
      "-0.1535198763934634\n"
     ]
    }
   ],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = data_with_dummies.corr()\n",
    "corr_results = corr['default_payment_next_month']\n",
    "print(corr_results)\n",
    "print(corr_results.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution :\n",
    "The most correlated features are the ones with the biggest and smallest correlation. All the ones with a correlation close to 0 are uncorrelated.\n",
    "\n",
    "The most correlated ones are therefore PAY_1 to 6\n",
    "\n",
    "We consider also the ones with less correlation to get a more precise predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Choosing Performance Metric and Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** What could be a fair metric for this problem? discuss different options starting from the plain accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Consider the case where distribution of different classes are not uniform. Also consider the cut-off threshold to separate positive and negative classes. Find a popular metric which is not sensitive to class distributions as well as cut-off threshold and use it for the rest of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution :\n",
    "The accuracy is a very useful metric to measure the results of a model. After training on the Train set (for exemple 80% of the dataset) We would compare the classification predicted by the model on the Testing set(the rest of the dataset) with the true result of the testing set. We compute a Pourcentage of matching prediction. The higher the accuracy is, the better our model performs. \n",
    "To Resume it is a good way to measure the results of a model. The only problem is that it is misleading when the distribution between classes is skewed.\n",
    " \n",
    "We also know that : Sampling differences can affect the final performance of the model, especially when tested against a small holdout set of testing data. Therefore, the models might perform very differently on a different population.\n",
    "\n",
    "Therefore in our case accuracy might not be the best way to evaluate our model. \n",
    "\n",
    "We have a few technics to improve our evaluation step. \n",
    "\n",
    "We can use the K-flod cross validation. It will evaluate our model with different training and testing set. We then can at the end perform the average of our result to get a proper evaluation step. It will avoid problem raising from cut-off threshold for testing/training data. \n",
    "\n",
    "We could also verify to have the same proportion of positive and negative data in the spliting dataset than in the Whole dataset\n",
    "\n",
    "Another good way to compare models even with different class distributions is computing the expected value as follow using the confusion metrix and the benefits metrix.\n",
    "Expected Value =\n",
    "    p(pos) * [ p(Y|pos)*b(Y,pos) + p(N|pos)*b(N,pos) ] +\n",
    "    p(neg) * [ p(N|neg)*b(N,neg) + p(Y|neg)*b(Y,neg) ]\n",
    "\n",
    "But for now we cannot compute the benefits metrix.\n",
    "\n",
    "Finally we can use The True Positive True Negative False Positive and False Negative  by computing the confusion metric to evaluate our results.\n",
    "From those 4 measures we can derive the F Score, F1 score, the recall and Precision of our model.\n",
    "\n",
    "#### With our Problem we are more interested by the Precision score of the positive class which means the precision of the prediction of the default of payment. Better ou model is Higher will this score be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHECK CONFUSION MATRIX CODE CHANGE THE TEXT ABOVE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_results(y_test,y_pred):\n",
    "    print ('confusion matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # calculate precision, recall and F score based on confusion metrics\n",
    "    # note for each class, metrics are calculated separately since average option is set to None \n",
    "    precision = precision_score(y_test, y_pred, average=None)\n",
    "    print ('precision: ', precision)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    print ('recall: ', recall)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "    print ('F score: ', f1)\n",
    "    print('The precision score of class 1(default next month) is :'+str(precision[1]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Come up with a sensible baseline to compare with your more complex models based on your selected metric. Print the baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Divide data into training and test sets using *train_test_split()* function in *sklearn.model_selection* package. Then fit an object from *DummyClassifier* class in *sklearn.dummy* package to training set and evaluate the performance on the test set. Keep the strategy of *DummyClassifier* object as default (stratified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_with_dummies.loc[:, ['LIMIT_BAL', 'EDUCATION', 'AGE', 'PAY_1', 'PAY_2', 'PAY_3',\n",
    "       'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3',\n",
    "       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2',\n",
    "       'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'SEX_1', 'SEX_2', 'MARRIAGE_0',\n",
    "       'MARRIAGE_1', 'MARRIAGE_2', 'MARRIAGE_3']]\n",
    "\n",
    "y = data_with_dummies['default_payment_next_month']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy : 0.6523333333333333\n",
      "confusion matrix: \n",
      " [[3628 1045]\n",
      " [1034  293]]\n",
      "precision:  [0.77820678 0.21898356]\n",
      "recall:  [0.77637492 0.22079879]\n",
      "F score:  [0.77728977 0.21988743]\n",
      "The precision score of class 1(default next month) is :0.21898355754857998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='stratified')\n",
    "dummy_clf.fit(X_train,y_train)\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "baseline_score = dummy_clf.score(X_test,y_test)\n",
    "print (\"Mean accuracy : \"+str(baseline_score))\n",
    "\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prediction using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Predict probability of default using logistic regression. Is normalization of data needed? What is the performance of your model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *LogisticRegression* class from *sklearn.linear_model* package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n",
    "\n",
    "It is always better to normalize the features before training a model. It allows the model to get better predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 70 µs, total: 137 ms\n",
      "Wall time: 137 ms\n",
      "Mean accuracy : 0.7788333333333334\n",
      "confusion matrix: \n",
      " [[4641   32]\n",
      " [1294   33]]\n",
      "precision:  [0.78197136 0.50769231]\n",
      "recall:  [0.99315215 0.02486812]\n",
      "F score:  [0.875      0.04741379]\n",
      "The precision score of class 1(default next month) is :0.5076923076923077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train_normalized = preprocessing.normalize(X_train,norm='max')\n",
    "X_test_normalized = preprocessing.normalize(X_test,norm='max')\n",
    "#print(X_train.head())\n",
    "lr_clf = LogisticRegression()\n",
    "%time lr_clf.fit(X_train_normalized,y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "lr_score = lr_clf.score(X_test_normalized,y_test)\n",
    "\n",
    "print (\"Mean accuracy : \"+str(lr_score))\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Try to think conceptually what is the correct way to estimate performance of a classifier if we need to tune hyper-parameters. Implement your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Think about the problem that might arise if we tune hyper-parameters using the training set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold crossvalidation is a very good way to estimate the performance of a classifier if we need to tune hyper-parameters. StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "We can do is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------FOLD1 ----------------------------\n",
      "fold 1: train set shape: (23999,) test set shape: (6001,)\n",
      "fold 1: train set: [ 5984  5985  5988 ... 29997 29998 29999] test set: [   0    1    2 ... 6044 6045 6047]\n",
      "CPU times: user 138 ms, sys: 442 µs, total: 138 ms\n",
      "Wall time: 138 ms\n",
      "Mean accuracy : 0.7788701883019497\n",
      "confusion matrix: \n",
      " [[4673    0]\n",
      " [1327    1]]\n",
      "precision:  [0.77883333 1.        ]\n",
      "recall:  [1.00000000e+00 7.53012048e-04]\n",
      "F score:  [0.87566757 0.00150489]\n",
      "The precision score of class 1(default next month) is :1.0\n",
      "-------------------FOLD2 ----------------------------\n",
      "fold 2: train set shape: (24000,) test set shape: (6000,)\n",
      "fold 2: train set: [    0     1     2 ... 29997 29998 29999] test set: [ 5984  5985  5988 ... 12049 12050 12051]\n",
      "CPU times: user 190 ms, sys: 159 ms, total: 350 ms\n",
      "Wall time: 161 ms\n",
      "Mean accuracy : 0.7788333333333334\n",
      "confusion matrix: \n",
      " [[4673    0]\n",
      " [1327    0]]\n",
      "precision:  [0.77883333 0.        ]\n",
      "recall:  [1. 0.]\n",
      "F score:  [0.87566757 0.        ]\n",
      "The precision score of class 1(default next month) is :0.0\n",
      "-------------------FOLD3 ----------------------------\n",
      "fold 3: train set shape: (24000,) test set shape: (6000,)\n",
      "fold 3: train set: [    0     1     2 ... 29997 29998 29999] test set: [11793 11794 11796 ... 18222 18223 18224]\n",
      "CPU times: user 171 ms, sys: 192 ms, total: 363 ms\n",
      "Wall time: 137 ms\n",
      "Mean accuracy : 0.7785\n",
      "confusion matrix: \n",
      " [[4671    2]\n",
      " [1327    0]]\n",
      "precision:  [0.77875959 0.        ]\n",
      "recall:  [0.99957201 0.        ]\n",
      "F score:  [0.87545685 0.        ]\n",
      "The precision score of class 1(default next month) is :0.0\n",
      "-------------------FOLD4 ----------------------------\n",
      "fold 4: train set shape: (24000,) test set shape: (6000,)\n",
      "fold 4: train set: [    0     1     2 ... 29997 29998 29999] test set: [17312 17315 17316 ... 24073 24074 24075]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/julien/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 117 ms, total: 318 ms\n",
      "Wall time: 198 ms\n",
      "Mean accuracy : 0.7788333333333334\n",
      "confusion matrix: \n",
      " [[4673    0]\n",
      " [1327    0]]\n",
      "precision:  [0.77883333 0.        ]\n",
      "recall:  [1. 0.]\n",
      "F score:  [0.87566757 0.        ]\n",
      "The precision score of class 1(default next month) is :0.0\n",
      "-------------------FOLD5 ----------------------------\n",
      "fold 5: train set shape: (24001,) test set shape: (5999,)\n",
      "fold 5: train set: [    0     1     2 ... 24073 24074 24075] test set: [23688 23691 23696 ... 29997 29998 29999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/julien/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 254 ms, sys: 78.6 ms, total: 332 ms\n",
      "Wall time: 255 ms\n",
      "Mean accuracy : 0.7787964660776796\n",
      "confusion matrix: \n",
      " [[4672    0]\n",
      " [1327    0]]\n",
      "precision:  [0.77879647 0.        ]\n",
      "recall:  [1. 0.]\n",
      "F score:  [0.87564427 0.        ]\n",
      "The precision score of class 1(default next month) is :0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/julien/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-Fold crossvalidation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "i = 1\n",
    "for train, test in skf.split(X,y):\n",
    "    print(\"-------------------FOLD\"+ str(i)+\" ----------------------------\")\n",
    "    print(\"fold %i: train set shape: %s test set shape: %s\" % (i, train.shape, test.shape))\n",
    "    print(\"fold %i: train set: %s test set: %s\" % (i, train, test))\n",
    "    #Select the appropriate part of the dataframe based on the array of index train and test\n",
    "    X_train_normalized = preprocessing.normalize(X.iloc[train,:],norm=\"max\")\n",
    "    y_train = y.iloc[train]\n",
    "    X_test_normalized = preprocessing.normalize(X.iloc[test,:],norm=\"max\")\n",
    "    y_test = y.iloc[test]\n",
    "    #print(y_train)\n",
    "    # Train our Model\n",
    "    %time lr_clf.fit(X_train_normalized,y_train)\n",
    "    # Predict our Results\n",
    "    y_pred = lr_clf.predict(X_test_normalized)\n",
    "    lr_score = lr_clf.score(X_test_normalized,y_test)\n",
    "    #Print our Results\n",
    "    print (\"Mean accuracy : \"+str(lr_score))\n",
    "    get_results(y_test,y_pred)\n",
    "    i+=1\n",
    "\n",
    "lr_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Version: print only the accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold\n",
      "[0.77874042 0.77874042 0.77874042 0.77874042 0.77866667 0.77833333\n",
      " 0.77892631 0.77892631 0.77925975 0.77892631]\n",
      "Mean Accuracy\n",
      "0.778800035900004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "cross_val = cross_val_score(LogisticRegression(), preprocessing.normalize(X,norm='max'), y, scoring='accuracy', cv=10)\n",
    "\n",
    "print(\"Accuracy for each fold\")\n",
    "print(cross_val)\n",
    "print(\"Mean Accuracy\")\n",
    "print(cross_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Tune the parameter *C* using *l1* penalty. Note that data normalization is required for regularized linear models. What is the correct method to implement the normalization ? Obtain the performance of the model using best value of *C*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Think about the possible problem that might arise if we standardize all data (training and test) and then fit a classifier to it. Read about *Pipeline* class from *sklearn.pipeline* package and see how taking advantage of *Pipeline* can avoid the mentioned problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution :\n",
    "The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators.\n",
    "It gives us the opportunity to apply the same transformation in the same order on our data each time.\n",
    "\n",
    "We will use standard scaler to normalize our data for regularized liner models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   19.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score is:  0.8113333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'regressor__C': 0.25}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#As for the regularization factor, we consider an exponential range of values\n",
    "alpha_to_test = 2.0**np.arange(-10, +6)\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', LogisticRegression(penalty='l1'))\n",
    "            ])\n",
    "\n",
    "params = {'regressor__C': alpha_to_test}\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))\n",
    "\n",
    "#pipe = pipe.fit(X_train, y_train)\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value of C is 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      " [[3773 1792]\n",
      " [ 165  270]]\n",
      "precision:  [0.95810056 0.13094083]\n",
      "recall:  [0.67798742 0.62068966]\n",
      "F score:  [0.79406503 0.21625951]\n",
      "The precision score of class 1(default next month) is :0.1309408341416101\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(penalty='l1',C=gridsearch.best_params_['regressor__C'])\n",
    "lr_clf.fit(X_train,y_train)\n",
    "y_test = lr_clf.predict(X_test)\n",
    "#print results\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** Check the coefficients and intercept of your tuned model under *l1* regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Extract the logistic regression from your pipeline and use its methods directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.25, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Coefficient : [[-0.10992314 -0.07768952  0.05230196  0.6516184   0.09809659  0.07943354\n",
      "   0.03713665  0.03573404  0.         -0.3295948   0.12258446  0.0804357\n",
      "   0.00253957  0.02317748  0.         -0.20866417 -0.24788842 -0.02932627\n",
      "  -0.04100516 -0.04659603 -0.02377498  0.00992231 -0.04738577 -0.05285821\n",
      "   0.02795191 -0.06923467  0.        ]]\n",
      "Intercept : [-1.46265186]\n"
     ]
    }
   ],
   "source": [
    "clf=gridsearch.best_estimator_.named_steps['regressor']\n",
    "print(clf)\n",
    "print(\"Coefficient : \"+str(clf.coef_))\n",
    "print(\"Intercept : \"+str(clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 5:** Draw the ROC curve of your tuned model under *l1* regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *roc_curve* function from *sklearn.metrics* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,)\n",
      "(6000,)\n",
      "false positive rates:  [0.         0.02567944 1.        ] \n",
      " true positive rates:  [0.         0.23737754 1.        ] \n",
      " thresholds:  [2 1 0]\n",
      "area under roc curve:  0.6058490541391962\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "y_pred = gridsearch.predict(X_test)\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)\n",
    "print ('false positive rates: ',fpr,'\\n','true positive rates: ',\n",
    "       tpr,'\\n','thresholds: ',thresholds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print ('area under roc curve: ', roc_auc)\n",
    "\n",
    "# plot roc curve\n",
    "plt.plot(fpr, tpr, color='red',lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Prediction using KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** What could be the advantage of knn over logistic regression? Shall the data be normalized ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution :\n",
    "\n",
    "- Training: k-nearest neighbors requires no training. Logistic regression requires some training.\n",
    "- Decision boundary: Logistic regression learns a linear classifier, while k-nearest neighbors can learn non-linear boundaries as well.\n",
    "\n",
    "\n",
    "\n",
    "Normalization : THe disctance is important to classify my featurs thus there is no need to normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Try to fit a knn classifier with 5 nearest nieghbors using training and test sets. Is normalization necessary ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *KNeighborsClassifier* class from *sklearn.neighbors* package.\n",
    "\n",
    "##### Solution :\n",
    "\n",
    "Normalization : THe disctance is important to classify my featurs thus there is no need to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy : 0.7533333333333333\n",
      "confusion matrix: \n",
      " [[4272  401]\n",
      " [1079  248]]\n",
      "precision:  [0.79835545 0.38212635]\n",
      "recall:  [0.91418789 0.18688772]\n",
      "F score:  [0.85235435 0.25101215]\n",
      "The precision score of class 1(default next month) is :0.38212634822804314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "n_neighbors = 5 \n",
    "clf_knn = KNeighborsClassifier(n_neighbors)\n",
    "#train\n",
    "clf_knn.fit(X_train, y_train)\n",
    "#predict\n",
    "y_pred = clf_knn.predict(X_test)\n",
    "lr_score = clf_knn.score(X_test,y_test)\n",
    "\n",
    "#print results\n",
    "print (\"Mean accuracy : \"+str(lr_score))\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Tune the parameter *n_neighbors* and report the perfromance of your tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 39 candidates, totalling 117 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 117 out of 117 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score is:  0.7801666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'regressor__n_neighbors': 36}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k to tetst\n",
    "k_to_test = np.arange(1, 40)\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('regressor', KNeighborsClassifier())\n",
    "            ])\n",
    "\n",
    "params = {'regressor__n_neighbors': k_to_test}\n",
    "\n",
    "gridsearch = GridSearchCV(pipe,params,n_jobs=4, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))\n",
    "\n",
    "#pipe = pipe.fit(X_train, y_train)\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      " [[4574   99]\n",
      " [1220  107]]\n",
      "precision:  [0.78943735 0.51941748]\n",
      "recall:  [0.97881447 0.08063301]\n",
      "F score:  [0.8739849  0.13959556]\n",
      "The precision score of class 1(default next month) is :0.5194174757281553\n"
     ]
    }
   ],
   "source": [
    "clf_knn2=KNeighborsClassifier(weights='uniform',n_neighbors=gridsearch.best_params_['regressor__n_neighbors'])\n",
    "clf_knn2.fit(X_train,y_train)\n",
    "y_pred=clf_knn2.predict(X_test)\n",
    "\n",
    "#print results\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** Apply knn classifier using *'weighted'* strategy which gives different weights to different neighbors based on their distance from the focal data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Modify the *weights* parameter of your knn classifier object, instantiated in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 39 candidates, totalling 117 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 117 out of 117 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score is:  0.779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'regressor__n_neighbors': 38}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k to tetst\n",
    "k_to_test = np.arange(1, 40)\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('regressor', KNeighborsClassifier(weights='distance'))\n",
    "            ])\n",
    "\n",
    "params = {'regressor__n_neighbors': k_to_test}\n",
    "\n",
    "gridsearch = GridSearchCV(pipe,params,n_jobs=4, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))\n",
    "\n",
    "#pipe = pipe.fit(X_train, y_train)\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      " [[4534  139]\n",
      " [1187  140]]\n",
      "precision:  [0.79251879 0.50179211]\n",
      "recall:  [0.97025465 0.10550113]\n",
      "F score:  [0.8724264 0.1743462]\n",
      "The precision score of class 1(default next month) is :0.5017921146953405\n"
     ]
    }
   ],
   "source": [
    "clf_knn2=KNeighborsClassifier(weights='distance',n_neighbors=gridsearch.best_params_['regressor__n_neighbors'])\n",
    "clf_knn2.fit(X_train,y_train)\n",
    "y_pred=clf_knn2.predict(X_test)\n",
    "#print results\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Prediction using Classifiers based on Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** How do you think about the required preprocessing for decision tree ? In particular discuss the case of one hot encoding and normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution :\n",
    "- Preprocessing : ...\n",
    "    -One Hot Encoded\n",
    "- Normalization : ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Fit a decision tree classifier to your data, with no hyper-parameter tuning, using training and test sets and report the performance of your model. Do the same with a different train and test set. How do the results compare? What are the most important features in either of the cases? Visualize your second tree classifier using *sklearn.tree.export_graphviz* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *DecisionTreeClassifier* class from *sklearn.tree* package. The *feature_importances_* attribute of this classs, reports the importance of different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_features(clf,n_importance):\n",
    "    print(\"Importance of features \"+str(clf.feature_importances_))\n",
    "    arr = np.array(clf.feature_importances_)\n",
    "    idx = (-arr).argsort()[:n_importance]\n",
    "    print(\"The \"+str(n_importance)+\" most important features are \")\n",
    "    return X.columns[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy : 0.731\n",
      "confusion matrix: \n",
      " [[3853  820]\n",
      " [ 794  533]]\n",
      "precision:  [0.82913708 0.39393939]\n",
      "recall:  [0.82452386 0.40165787]\n",
      "F score:  [0.82682403 0.39776119]\n",
      "The precision score of class 1(default next month) is :0.3939393939393939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "decTree_clf = DecisionTreeClassifier()\n",
    "decTree_clf.fit(X=X_train,y=y_train)\n",
    "y_pred = decTree_clf.predict(X_test)\n",
    "\n",
    "#print results\n",
    "score = decTree_clf.score(X_test,y_test)\n",
    "print (\"Mean accuracy : \"+str(score))\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of features [0.05600363 0.02274681 0.07497557 0.16151301 0.03444798 0.01148501\n",
      " 0.01096667 0.01071479 0.01151093 0.06516502 0.04981126 0.04386343\n",
      " 0.04213452 0.04193262 0.05097436 0.04733499 0.04403153 0.05937182\n",
      " 0.03327306 0.04560955 0.05207903 0.00682142 0.00780248 0.0007128\n",
      " 0.00673148 0.00651367 0.00147256]\n",
      "The 5 most important features are \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['PAY_1', 'AGE', 'BILL_AMT1', 'PAY_AMT3', 'LIMIT_BAL'], dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_features(decTree_clf,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy : 0.7298333333333333\n",
      "confusion matrix: \n",
      " [[3803  870]\n",
      " [ 751  576]]\n",
      "precision:  [0.83509003 0.39834025]\n",
      "recall:  [0.8138241  0.43406179]\n",
      "F score:  [0.82431993 0.41543455]\n",
      "The precision score of class 1(default next month) is :0.3983402489626556\n",
      "Importance of features [0.06847666 0.0192049  0.06703385 0.16157826 0.03301541 0.00693859\n",
      " 0.00862445 0.00889787 0.01231635 0.06475555 0.05190677 0.04543107\n",
      " 0.04396475 0.04167039 0.05078537 0.04715333 0.05175381 0.05743657\n",
      " 0.03718585 0.04145127 0.05109278 0.00681504 0.00581707 0.00092672\n",
      " 0.00635947 0.00722212 0.00218574]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "decTree_clf = DecisionTreeClassifier()\n",
    "decTree_clf.fit(X=X_train,y=y_train)\n",
    "y_pred = decTree_clf.predict(X_test)\n",
    "\n",
    "#print results\n",
    "score = decTree_clf.score(X_test,y_test)\n",
    "print (\"Mean accuracy : \"+str(score))\n",
    "get_results(y_test,y_pred)\n",
    "print(\"Importance of features \"+str(decTree_clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of features [0.06847666 0.0192049  0.06703385 0.16157826 0.03301541 0.00693859\n",
      " 0.00862445 0.00889787 0.01231635 0.06475555 0.05190677 0.04543107\n",
      " 0.04396475 0.04167039 0.05078537 0.04715333 0.05175381 0.05743657\n",
      " 0.03718585 0.04145127 0.05109278 0.00681504 0.00581707 0.00092672\n",
      " 0.00635947 0.00722212 0.00218574]\n",
      "The 5 most important features are \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['PAY_1', 'LIMIT_BAL', 'AGE', 'BILL_AMT1', 'PAY_AMT3'], dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_features(decTree_clf,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both results are very Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-93eec6ce8c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(decTree_clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_call\n",
    "check_call(['dot','-Tpng','tree.dot','-o','OutputFile.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Fit a random forest classifier to your data by adjusting the total number of trees. How do your results compare to a single decision tree? report importance of different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *RandomForestClassifier* class from *sklearn.ensemble* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 199 candidates, totalling 597 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  58 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=4)]: Done 208 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done 458 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=4)]: Done 597 out of 597 | elapsed: 27.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score is:  0.8166666666666667\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-d166a26bf09f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final score is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrandTree_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgridsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regressor__n_estimators'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mrandTree_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandTree_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_neighbors'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "n_to_test = np.arange(1, 200)\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('regressor', RandomForestClassifier())\n",
    "            ])\n",
    "\n",
    "params = {'regressor__n_estimators': n_to_test}\n",
    "\n",
    "gridsearch = GridSearchCV(pipe,params,n_jobs=4, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy : 0.8158333333333333\n",
      "confusion matrix: \n",
      " [[4392  281]\n",
      " [ 824  503]]\n",
      "precision:  [0.84202454 0.64158163]\n",
      "recall:  [0.93986732 0.37905049]\n",
      "F score:  [0.88825968 0.4765514 ]\n",
      "The precision score of class 1(default next month) is :0.6415816326530612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "randTree_clf = RandomForestClassifier(n_estimators=gridsearch.best_params_['regressor__n_estimators'])\n",
    "randTree_clf.fit(X=X_train,y=y_train)\n",
    "y_pred = randTree_clf.predict(X_test)\n",
    "\n",
    "#print results\n",
    "score = randTree_clf.score(X_test,y_test)\n",
    "print (\"Mean accuracy : \"+str(score))\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_['regressor__n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of features [0.05931126 0.01995874 0.0656581  0.09933462 0.0472319  0.02317351\n",
      " 0.02455875 0.02198946 0.01768895 0.06037702 0.05268554 0.05079133\n",
      " 0.04961764 0.04915152 0.04852466 0.05021566 0.04691439 0.04523865\n",
      " 0.04285169 0.04285532 0.04570146 0.00843819 0.00871725 0.00033025\n",
      " 0.00856199 0.00843093 0.00169121]\n",
      "The 5 most important features are \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['PAY_1', 'AGE', 'BILL_AMT1', 'LIMIT_BAL', 'BILL_AMT2'], dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_features(randTree_clf,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** Fit a gradient boosted tree classifier to your data by tunning the total number of trees. How do your results compare to the other models? What are the most important features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *GradientBoostingClassifier* class from *sklearn.ensemble* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy : 0.8205\n",
      "confusion matrix: \n",
      " [[4445  228]\n",
      " [ 849  478]]\n",
      "precision:  [0.83962977 0.67705382]\n",
      "recall:  [0.95120907 0.360211  ]\n",
      "F score:  [0.89194341 0.47024102]\n",
      "The precision score of class 1(default next month) is :0.6770538243626062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "GradTree_clf = GradientBoostingClassifier()\n",
    "GradTree_clf.fit(X=X_train,y=y_train)\n",
    "y_pred = GradTree_clf.predict(X_test)\n",
    "\n",
    "#print results\n",
    "score = GradTree_clf.score(X_test,y_test)\n",
    "print (\"Mean accuracy : \"+str(score))\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of features [0.06620478 0.02172896 0.04125565 0.15261758 0.02794735 0.02883496\n",
      " 0.02214051 0.03128873 0.02496172 0.11314753 0.05855562 0.04166046\n",
      " 0.03623507 0.03687666 0.03771943 0.05383595 0.0403825  0.03197003\n",
      " 0.02026025 0.05029455 0.02428331 0.0104192  0.00210666 0.00777023\n",
      " 0.0093936  0.0081087  0.        ]\n",
      "The 6 most important features are \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['PAY_1', 'BILL_AMT1', 'LIMIT_BAL', 'BILL_AMT2', 'PAY_AMT1', 'PAY_AMT5'], dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_features(GradTree_clf,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is the best classifier we have so far !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Feature Selection / Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Fit a logistic regression (with no l1 and l2 regularization) to the top 6 important features obtained from Q 4 of part 5. How do you compare the performance of this model to the logistic regression model trained on all features ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PAY_1  BILL_AMT1  LIMIT_BAL  PAY_AMT1  BILL_AMT2  PAY_AMT5\n",
      "0      2       3913      20000         0       3102         0\n",
      "1     -1       2682     120000         0       1725         0\n",
      "2      0      29239      90000      1518      14027      1000\n",
      "3      0      46990      50000      2000      48233      1069\n",
      "4     -1       8617      50000      2000       5670       689\n",
      "Mean accuracy : 0.7788333333333334\n",
      "confusion matrix: \n",
      " [[3203 1470]\n",
      " [ 735  592]]\n",
      "precision:  [0.81335703 0.2870999 ]\n",
      "recall:  [0.68542692 0.44611907]\n",
      "F score:  [0.74393218 0.34936559]\n",
      "The precision score of class 1(default next month) is :0.2870999030067895\n"
     ]
    }
   ],
   "source": [
    "X_6 = X.loc[:,['PAY_1', 'BILL_AMT1', 'LIMIT_BAL', 'PAY_AMT1', 'BILL_AMT2', 'PAY_AMT5']]\n",
    "\n",
    "print(X_6.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_6, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "\n",
    "X_train_normalized = preprocessing.normalize(X_train,norm='max')\n",
    "X_test_normalized = preprocessing.normalize(X_test,norm='max')\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_normalized,y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "lr_score = lr_clf.score(X_test_normalized,y_test)\n",
    "\n",
    "print (\"Mean accuracy : \"+str(lr_score))\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are slightly differente. The overral accuracy is the same but the precision of the class 1 is way better when training on all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Apply PCA to project your data into 6 dimensions and fit a logistic regression as Q 1. How you compare the performance of your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *PCA* class from *sklearn.decomposition* package. Create a pipeline consists of *StandardScaler*, *PCA* and *LogisticRegression* classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score is:  0.8116666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reduce_dim': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#As for the regularization factor, we consider an exponential range of values\n",
    "#alpha_to_test = 2.0**np.arange(-10, +6)\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reduce_dim',PCA(n_components=6)),\n",
    "            ('regressor', LogisticRegression())\n",
    "            ])\n",
    "params = {'reduce_dim': [PCA()]}\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))\n",
    "\n",
    "#pipe = pipe.fit(X_train, y_train)\n",
    "gridsearch.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      " [[3772 1789]\n",
      " [ 166  273]]\n",
      "precision:  [0.95784662 0.13239573]\n",
      "recall:  [0.67829527 0.62186788]\n",
      "F score:  [0.79418886 0.21831267]\n",
      "The precision score of class 1(default next month) is :0.13239573229873908\n"
     ]
    }
   ],
   "source": [
    "y_test = gridsearch.predict(X_test)\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Visualize your data in 2 dimensions using PCA. What is the disadvantage of PCA when used for visualizing high-dimensional data? What could be a better solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Don't forget to normalize your data before applying PCA. Use *matplotlib.pyplot.scatter* function to plot your data in two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution : \n",
    "The disadvantage of PCA when using visualization is that the axis don't not represent any comprehensible measure.\n",
    "We could instead use different plot with only 2 or 3 features each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We assume that we have to vizualize our features in 2 dimensions\n",
    "\n",
    "# normalize\n",
    "X_norm = preprocessing.normalize(X,norm='max')\n",
    "#pca\n",
    "pca = PCA(n_components=2)\n",
    "data = pca.fit(X)\n",
    "#plot\n",
    "plt.scatter(data.components_[0],data.components_[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Prediction Improvement: Better Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** If model A performas better than all other models for our problem, does it mean that it is in general a better model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Google *no-free-lunch* theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution :\n",
    "What the NFLT is trying to tell us is, we are generally not going to find off the shelf algorithms that fit perfectly to our data. We are going to have to architect the algorithm to better fit the data — for example, make a Neural Network recurrent to better fit a time series, or make an Multi Layer Perceptron into a Convolutional Neural Network to better understand the spacial information in the input data. \n",
    "Thus a composition of different model might be a better solution than individual model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Create a simple ensmble classifier using average predicted probabilities of  logistic regression (with l1 penalty), knn classifier, random forest classifier and gradient boosted classifier. Compare its performance with each of the 4 individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2)\n",
      "[[0.88151379 0.11848621]\n",
      " [0.79969091 0.20030909]\n",
      " [0.91492909 0.08507091]\n",
      " ...\n",
      " [0.86914187 0.13085813]\n",
      " [0.8950265  0.1049735 ]\n",
      " [0.96135959 0.03864041]]\n"
     ]
    }
   ],
   "source": [
    "#lr\n",
    "lr_clf = LogisticRegression(penalty='l1',C=gridsearch.best_params_['regressor__C'])\n",
    "lr_clf.fit(X_train,y_train)\n",
    "#knn\n",
    "clf_knn2=KNeighborsClassifier(weights='uniform',n_neighbors=36)\n",
    "clf_knn2.fit(X_train,y_train)\n",
    "#random forest\n",
    "randTree_clf = RandomForestClassifier()\n",
    "randTree_clf.fit(X=X_train,y=y_train)\n",
    "#gradient boosted\n",
    "GradTree_clf = GradientBoostingClassifier()\n",
    "GradTree_clf.fit(X=X_train,y=y_train)\n",
    "y_pred_ensemble = (GradTree_clf.predict_proba(X_test)+randTree_clf.predict_proba(X_test)+lr_clf.predict_proba(X_test)+clf_knn2.predict_proba(X_test))/4\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The accuracy score of the ensemble classifier is \n",
      "0.8051666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\" The accuracy score of the ensemble classifier is \")\n",
    "print(np.mean([GradTree_clf.score(X_test,y_test),randTree_clf.score(X_test,y_test),lr_clf.score(X_test,y_test),clf_knn2.score(X_test,y_test)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is similar to random forest lower than lr with pca(6) and gradientboost and greater than knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** How can you make your performance results statistically more reliable ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Consider selection bias in both tuning and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Food for Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read: https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To watch: https://www.ted.com/talks/deb_roy_the_birth_of_a_word#t-316846"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
